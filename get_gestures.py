import cv2
import mediapipe as mp
import numpy as np
import json
import os
import time
from sklearn.neighbors import KNeighborsClassifier
import joblib

'''
handtype:
0   ğŸ¤˜
1   ğŸ‘
2   âœŒï¸
3   ğŸ‘Œ
4   ğŸ¤™
5   ğŸ¤Œ
'''
# åˆå§‹åŒ–gesture_code
gesture_code = 10
# åˆå§‹åŒ–MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

# åˆ›å»ºHandå¯¹è±¡
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)

# æ‰“å¼€æ‘„åƒå¤´
cap = cv2.VideoCapture(0)

# ä¿å­˜è·¯å¾„
save_path = "my_gestures/data_"+f'{gesture_code}'+"/"  # æ›¿æ¢ä¸ºä½ æƒ³ä¿å­˜çš„è·¯å¾„

# åˆ›å»ºä¿å­˜è·¯å¾„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(save_path, exist_ok=True)

# åˆå§‹åŒ–è®¡æ—¶å™¨
start_time = time.time()

def normalization(landmarks):
    # å°†å…³é”®ç‚¹è½¬æ¢ä¸ºç›¸å¯¹äºæ‰‹éƒ¨ä¸­å¿ƒç‚¹çš„åæ ‡ç³»
    landmarks = np.array(landmarks)
    center = np.mean(landmarks, axis=0)
    normalized_landmarks = landmarks - center
    max_value = np.max(np.linalg.norm(normalized_landmarks, axis=1))
    normalized_landmarks /= max_value
    return normalized_landmarks

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # ç¿»è½¬å›¾åƒ
    frame = cv2.flip(frame, 1)
    # å°†å›¾åƒè½¬æ¢ä¸ºRGB
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # æ£€æµ‹æ‰‹éƒ¨
    results = hands.process(rgb_frame)
    
    # è·å–å½“å‰æ—¶é—´
    current_time = time.time()
    
    if results.multi_hand_landmarks:
        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
            # è·å–æ‰‹çš„ç±»å‹ï¼ˆå·¦æ‰‹æˆ–å³æ‰‹ï¼‰
            hand_type = results.multi_handedness[idx].classification[0].label
            
            # è‡ªå®šä¹‰ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹å’Œè¿æ¥çº¿
            mp_drawing.draw_landmarks(
                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())
            
            # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºæ‰‹çš„ç±»å‹
            cv2.putText(frame, hand_type, 
                        (int(hand_landmarks.landmark[0].x * frame.shape[1]), 
                         int(hand_landmarks.landmark[0].y * frame.shape[0])), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
            
            # æ¯ç§’ä¿å­˜ä¸€æ¬¡
            if current_time - start_time >= 0.5:
                start_time = current_time  # é‡ç½®è®¡æ—¶å™¨
                
                # ä¿å­˜å½“å‰å¸§å›¾åƒ
                image_filename = os.path.join(save_path, f"frame_{int(current_time)}.png")
                cv2.imwrite(image_filename, frame)
                
                # ä¿å­˜å…³é”®ç‚¹ä¿¡æ¯
                landmarks = [{
                    "x": landmark.x,
                    "y": landmark.y,
                    "z": landmark.z
                } for landmark in hand_landmarks.landmark]
                normalized_landmarks = normalization([[lm["x"], lm["y"], lm["z"]] for lm in landmarks])
                print('normalized')
                print(normalized_landmarks)
                # é‡æ–°æŒ‰ç…§åŸæ ¼å¼ä¿å­˜å½’ä¸€åŒ–åçš„å…³é”®ç‚¹
                json_data = {
                    "hand_type": gesture_code,
                    "landmarks": [{"x": float(lm[0]), "y": float(lm[1]), "z": float(lm[2])} for lm in normalized_landmarks]
                }
                print('jsonfied')
                json_filename = os.path.join(save_path, f"frame_{int(current_time)}.json")
                with open(json_filename, 'w') as json_file:
                    json.dump(json_data, json_file, indent=4)
    
    # æ˜¾ç¤ºå›¾åƒ
    cv2.imshow('Hand Gesture Recognition', frame)
    
    if cv2.waitKey(1) & 0xFF == 27:
        break

cap.release()
cv2.destroyAllWindows()
hands.close()
